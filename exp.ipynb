{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_raster_to_slu(raster, slope_units, reduce_fn=np.mean, get_im=False):\n",
    "    raster = raster.astype(np.float32)\n",
    "    n_units = len(np.unique(slope_units))\n",
    "    feature_table = np.zeros(n_units, dtype=raster.dtype)\n",
    "    for i in range(n_units):\n",
    "        if np.any(slope_units == i+1) == 0:\n",
    "            continue\n",
    "        feature_table[i] = reduce_fn(raster[slope_units == i+1])\n",
    "    if get_im:\n",
    "        feature_im = np.zeros_like(raster)\n",
    "        for i in range(n_units):\n",
    "            if np.any(slope_units == i+1) == 0:\n",
    "                continue\n",
    "            feature_im[slope_units == i+1] = feature_table[i]\n",
    "        return feature_table, feature_im\n",
    "    return feature_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_region_slopeunits(raw_dir, region_id, min_count, out_dir):\n",
    "\n",
    "    with open(f'{raw_dir}/{region_id}.pkl', 'rb') as f:\n",
    "        region = pickle.load(f)\n",
    "\n",
    "    slopeunits = region['slopeunits']\n",
    "    y = region['inventory']\n",
    "    X = region['features']\n",
    "    feature_names = region['names']\n",
    "    metadata = region['metadata']\n",
    "\n",
    "    y_slu = map_raster_to_slu(y, slopeunits, reduce_fn=np.mean)\n",
    "    counts_slu = map_raster_to_slu(np.ones_like(y), slopeunits, reduce_fn=np.sum)\n",
    "\n",
    "    extreme_features = ['slope', 'curv_mean', 'curv_total', 'curv_profile', 'drainage_area']\n",
    "    feats, new_feat_names = [], []\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        feat_mean = map_raster_to_slu(X[i], slopeunits, reduce_fn=np.mean)\n",
    "        feat_var = map_raster_to_slu(X[i], slopeunits, reduce_fn=np.var)\n",
    "        feats += [feat_mean, feat_var]\n",
    "        new_feat_names += [f'{feat}_mean', f'{feat}_var']\n",
    "        if feat in extreme_features:\n",
    "            feat_min = map_raster_to_slu(X[i], slopeunits, reduce_fn=np.min)\n",
    "            feat_max = map_raster_to_slu(X[i], slopeunits, reduce_fn=np.max)\n",
    "            feats += [feat_min, feat_max]\n",
    "            new_feat_names += [f'{feat}_min', f'{feat}_max']\n",
    "    feats = np.stack(feats)\n",
    "\n",
    "    mask = counts_slu >= min_count\n",
    "    feats = feats[:,mask]\n",
    "    y_slu = y_slu[mask]\n",
    "    counts_slu = counts_slu[mask]\n",
    "    X_slu = pd.DataFrame(feats, index=new_feat_names).T\n",
    "\n",
    "    data_dict = {\n",
    "        'X': X_slu,\n",
    "        'y': y_slu,\n",
    "        'counts': counts_slu,\n",
    "        'slope_units': slopeunits,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "\n",
    "    with open(f'{out_dir}/{region_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "with open('../wenchuan/processed_data/box_1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2971"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['kept_su_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {}\n",
    "for i in range(1, 8):\n",
    "    with open(f'../wenchuan/processed_data/box_{i}.pkl', 'rb') as f:\n",
    "        data[f'box_{i}'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [len(data[key]['X']) for key in data]\n",
    "num_landslides = [np.mean(data[key]['y']) for key in data]\n",
    "names = [key for key in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Number of SUs</th>\n",
       "      <th>Mean Landslide Coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>box_1</td>\n",
       "      <td>2971</td>\n",
       "      <td>0.173895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>box_2</td>\n",
       "      <td>3048</td>\n",
       "      <td>0.052195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>box_3</td>\n",
       "      <td>2359</td>\n",
       "      <td>0.057619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>box_4</td>\n",
       "      <td>2214</td>\n",
       "      <td>0.222901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>box_5</td>\n",
       "      <td>2617</td>\n",
       "      <td>0.014104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>box_6</td>\n",
       "      <td>1782</td>\n",
       "      <td>0.030834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>box_7</td>\n",
       "      <td>3245</td>\n",
       "      <td>0.049984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Region  Number of SUs  Mean Landslide Coverage\n",
       "0  box_1           2971                 0.173895\n",
       "1  box_2           3048                 0.052195\n",
       "2  box_3           2359                 0.057619\n",
       "3  box_4           2214                 0.222901\n",
       "4  box_5           2617                 0.014104\n",
       "5  box_6           1782                 0.030834\n",
       "6  box_7           3245                 0.049984"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Region': names, 'Number of SUs': counts, 'Mean Landslide Coverage': num_landslides})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "\n",
    "def generate_grid(bounds, cell_size):\n",
    "    \"\"\"\n",
    "    Generate a list of square polygons (fishnet) covering the bounding box.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bounds : tuple (minx, miny, maxx, maxy)\n",
    "        Bounding box of the geometry.\n",
    "    cell_size : float\n",
    "        The desired size of each grid cell (square).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of shapely.geometry.Polygon\n",
    "        List of square polygons.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = bounds\n",
    "    \n",
    "    # Generate the x and y coordinates of the grid cells\n",
    "    x_coords = np.arange(minx, maxx, cell_size)\n",
    "    y_coords = np.arange(miny, maxy, cell_size)\n",
    "    \n",
    "    # Create polygons for each cell in the grid\n",
    "    polygons = []\n",
    "    for x in x_coords:\n",
    "        for y in y_coords:\n",
    "            # Coordinates of the square cell\n",
    "            x2 = x + cell_size\n",
    "            y2 = y + cell_size\n",
    "            \n",
    "            # Create the polygon\n",
    "            square = Polygon([\n",
    "                (x, y),\n",
    "                (x2, y),\n",
    "                (x2, y2),\n",
    "                (x, y2)\n",
    "            ])\n",
    "            polygons.append(square)\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "def split_into_grid(original_poly, cell_size):\n",
    "    \"\"\"\n",
    "    Split a single polygon (original_poly) into smaller squares of size cell_size,\n",
    "    preserving the original boundary where intersections occur.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    original_poly : shapely.geometry.Polygon or MultiPolygon\n",
    "    cell_size : float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.GeoDataFrame\n",
    "        GeoDataFrame containing the intersected squares.\n",
    "    \"\"\"\n",
    "    # 1. Get the bounding box of the polygon\n",
    "    bounds = original_poly.bounds\n",
    "    \n",
    "    # 2. Generate the grid (fishnet) of squares\n",
    "    squares = generate_grid(bounds, cell_size)\n",
    "    \n",
    "    # 3. Intersect each grid square with the original polygon\n",
    "    intersected_squares = []\n",
    "    for sq in squares:\n",
    "        intersected = sq.intersection(original_poly)\n",
    "        if not intersected.is_empty:\n",
    "            intersected_squares.append(intersected)\n",
    "    \n",
    "    # 4. Convert to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(geometry=intersected_squares, crs=\"EPSG:4326\")\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polygon shapefile\n",
    "# Replace 'your_shapefile.shp' with your actual shapefile path\n",
    "gdf_poly = gpd.read_file('~/Desktop/Wenchuan/polygons/region_bound.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/rc950kx52952n0613q25df340000gn/T/ipykernel_93069/4254012442.py:3: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  original_poly = gdf_poly.unary_union  # This merges all polygons into one\n"
     ]
    }
   ],
   "source": [
    "# If the shapefile has multiple polygons, you might iterate over each row.\n",
    "# For demonstration, assume there is only one (or that we dissolve them).\n",
    "original_poly = gdf_poly.union_all()  # This merges all polygons into one\n",
    "\n",
    "# Desired cell size (in the same units as your shapefile)\n",
    "cell_size = 1000  # for example, 1,000 units on each side\n",
    "\n",
    "# Split into grid\n",
    "result_gdf = split_into_grid(original_poly, cell_size)\n",
    "\n",
    "# Optionally save to a new shapefile\n",
    "result_gdf.to_file('~/Desktop/Wenchuan/polygons/split_grid_region_bound.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def compute_slopeunit_centroids(raster_path):\n",
    "    \"\"\"\n",
    "    Reads a slope-units raster from 'raster_path', where each slope unit\n",
    "    is identified by a unique integer ID. Computes the centroid\n",
    "    (x, y) in map coordinates for each ID.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids : dict\n",
    "        Dictionary keyed by slope-unit ID, with values = (x, y) centroid.\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        slope_units = src.read(1)  # read the first (and only) band\n",
    "        transform = src.transform\n",
    "        # If needed, check src.crs for the coordinate system\n",
    "\n",
    "    # Identify unique nonzero IDs (assuming 0 or nodata means \"no slope unit\")\n",
    "    unique_ids = np.unique(slope_units)\n",
    "    unique_ids = unique_ids[unique_ids != 0]  # filter out 0 if present\n",
    "\n",
    "    centroids = np.zeros((len(unique_ids), 2), dtype=np.float64)\n",
    "    for uid in unique_ids:\n",
    "        # Get all pixels belonging to this slope unit\n",
    "        rows, cols = np.where(slope_units == uid)\n",
    "        if len(rows) == 0:\n",
    "            continue  # skip if empty for some reason\n",
    "        \n",
    "        # Compute the mean row and column\n",
    "        mean_row = rows.mean()\n",
    "        mean_col = cols.mean()\n",
    "        \n",
    "        # Convert (row,col) → (x,y) using the geotransform\n",
    "        # You can do this manually or use rasterio's transform machinery:\n",
    "        # x, y = transform * (col, row)\n",
    "        x, y = rasterio.transform.xy(transform, mean_row, mean_col, offset='center')\n",
    "        \n",
    "        centroids[uid-1] = (x, y)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = '/Users/arushramteke/Desktop/Wenchuan/processed_regions/box_1/slopeunits.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(raster_path) as src:\n",
    "    slope_units = src.read(1)  # read the first (and only) band\n",
    "    transform = src.transform\n",
    "    # If needed, check src.crs for the coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = compute_slopeunit_centroids(raster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/arushramteke/Desktop/Wenchuan/pipeline_dump/box_1.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 910, 752)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4865, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['centroids'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
